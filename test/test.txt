import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Literal
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import pandas as pd
from enum import Enum

class DatasetType(Enum):
    ESO = "eso"
    GER = "ger"

@dataclass
class ModelConfig:
    dataset_type: DatasetType
    input_size: int = field(init=False)  # Will be set based on dataset_type
    hidden_size: int = 128
    num_layers: int = 2
    output_size: int = 1
    dropout: float = 0.2
    batch_size: int = 32
    epochs: int = 50
    learning_rate: float = 0.001
    lr_decay: float = 0.95
    sequence_length: int = 24  # Number of time steps to look back
    n_splits: int = 5  # Number of folds for cross validation
    
    def __post_init__(self):
        # Set input size based on dataset type
        self.input_size = 86 if self.dataset_type == DatasetType.ESO else 67

class EnergyDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray, sequence_length: int):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
        self.sequence_length = sequence_length

    def __len__(self):
        return len(self.X) - self.sequence_length + 1

    def __getitem__(self, idx):
        x_sequence = self.X[idx:idx + self.sequence_length]
        y_target = self.y[idx + self.sequence_length - 1]
        return x_sequence, y_target

class LSTMModel(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size=config.input_size,
            hidden_size=config.hidden_size,
            num_layers=config.num_layers,
            batch_first=True,
            dropout=config.dropout
        )
        self.fc = nn.Linear(config.hidden_size, config.output_size)
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        predictions = self.fc(lstm_out[:, -1, :])
        return predictions

@dataclass
class Modeler:
    config: ModelConfig
    device: torch.device = field(default_factory=lambda: torch.device('cuda' if torch.cuda.is_available() else 'cpu'))
    model: Optional[LSTMModel] = None
    criterion: Optional[nn.Module] = None
    optimizer: Optional[torch.optim.Optimizer] = None
    scheduler: Optional[torch.optim.lr_scheduler.StepLR] = None
    
    def __post_init__(self):
        self.model = LSTMModel(self.config).to(self.device)
        self.criterion = nn.MSELoss()
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.config.learning_rate)
        self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimizer, gamma=self.config.lr_decay)

    def prepare_data(self, X: pd.DataFrame, y: pd.DataFrame) -> Tuple[DataLoader, np.ndarray]:
        # Remove datetime column and convert to numpy arrays
        X_values = X.drop('ID', axis=1).values
        y_values = y['FOSSIL'].values.reshape(-1, 1)
        
        # Create dataset and dataloader
        dataset = EnergyDataset(X_values, y_values, self.config.sequence_length)
        dataloader = DataLoader(dataset, batch_size=self.config.batch_size, shuffle=True)
        
        return dataloader, y_values

    def train_epoch(self, train_loader: DataLoader) -> float:
        self.model.train()
        total_loss = 0
        
        for batch_X, batch_y in train_loader:
            batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
            
            self.optimizer.zero_grad()
            predictions = self.model(batch_X)
            loss = self.criterion(predictions, batch_y)
            
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item()
            
        return total_loss / len(train_loader)

    def validate(self, val_loader: DataLoader) -> Tuple[float, np.ndarray, np.ndarray]:
        self.model.eval()
        total_loss = 0
        predictions = []
        actuals = []
        
        with torch.no_grad():
            for batch_X, batch_y in val_loader:
                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)
                batch_predictions = self.model(batch_X)
                
                loss = self.criterion(batch_predictions, batch_y)
                total_loss += loss.item()
                
                predictions.extend(batch_predictions.cpu().numpy())
                actuals.extend(batch_y.cpu().numpy())
        
        return total_loss / len(val_loader), np.array(predictions), np.array(actuals)

    def cross_validate(self, X: pd.DataFrame, y: pd.DataFrame) -> dict:
        kfold = KFold(n_splits=self.config.n_splits, shuffle=True, random_state=42)
        cv_scores = {
            'mse': [], 'mae': [], 'r2': [],
            'train_losses': [], 'val_losses': []
        }
        
        X_values = X.drop('ID', axis=1).values
        y_values = y['FOSSIL'].values
        
        for fold, (train_idx, val_idx) in enumerate(kfold.split(X_values)):
            print(f"Fold {fold + 1}/{self.config.n_splits}")
            
            # Prepare data for this fold
            X_train, X_val = X_values[train_idx], X_values[val_idx]
            y_train, y_val = y_values[train_idx], y_values[val_idx]
            
            train_dataset = EnergyDataset(X_train, y_train.reshape(-1, 1), self.config.sequence_length)
            val_dataset = EnergyDataset(X_val, y_val.reshape(-1, 1), self.config.sequence_length)
            
            train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=self.config.batch_size)
            
            # Reset model for each fold
            self.__post_init__()
            
            # Training loop
            train_losses = []
            val_losses = []
            
            for epoch in range(self.config.epochs):
                train_loss = self.train_epoch(train_loader)
                val_loss, predictions, actuals = self.validate(val_loader)
                
                train_losses.append(train_loss)
                val_losses.append(val_loss)
                
                self.scheduler.step()
                
                if (epoch + 1) % 10 == 0:
                    print(f"Epoch {epoch + 1}/{self.config.epochs}, "
                          f"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
            
            # Calculate metrics
            mse = mean_squared_error(actuals, predictions)
            mae = mean_absolute_error(actuals, predictions)
            r2 = r2_score(actuals, predictions)
            
            cv_scores['mse'].append(mse)
            cv_scores['mae'].append(mae)
            cv_scores['r2'].append(r2)
            cv_scores['train_losses'].append(train_losses)
            cv_scores['val_losses'].append(val_losses)
            
        # Calculate and print average scores
        print(f"\nCross-validation results for {self.config.dataset_type.value}:")
        print(f"Average MSE: {np.mean(cv_scores['mse']):.2f} ± {np.std(cv_scores['mse']):.2f}")
        print(f"Average MAE: {np.mean(cv_scores['mae']):.2f} ± {np.std(cv_scores['mae']):.2f}")
        print(f"Average R2: {np.mean(cv_scores['r2']):.4f} ± {np.std(cv_scores['r2']):.4f}")
        
        return cv_scores

    def predict(self, X_test: pd.DataFrame) -> np.ndarray:
        self.model.eval()
        test_dataset = EnergyDataset(
            X_test.drop('ID', axis=1).values,
            np.zeros((len(X_test), 1)),  # Dummy values for y
            self.config.sequence_length
        )
        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size)
        
        predictions = []
        with torch.no_grad():
            for batch_X, _ in test_loader:
                batch_X = batch_X.to(self.device)
                batch_predictions = self.model(batch_X)
                predictions.extend(batch_predictions.cpu().numpy())
        
        return np.array(predictions)

# Example usage for both ESO and GER predictions
def train_and_evaluate_models(
    X_train_eso_scaled: pd.DataFrame, y_train_eso: pd.DataFrame,
    X_test_eso_scaled: pd.DataFrame, y_test_eso: pd.DataFrame,
    X_train_ger_scaled: pd.DataFrame, y_train_ger: pd.DataFrame,
    X_test_ger_scaled: pd.DataFrame, y_test_ger: pd.DataFrame
) -> Tuple[dict, dict]:
    
    # Train and evaluate ESO model
    eso_config = ModelConfig(dataset_type=DatasetType.ESO)
    eso_modeler = Modeler(eso_config)
    print("Training ESO model...")
    eso_cv_scores = eso_modeler.cross_validate(X_train_eso_scaled, y_train_eso)
    eso_predictions = eso_modeler.predict(X_test_eso_scaled)
    
    # Train and evaluate GER model
    ger_config = ModelConfig(dataset_type=DatasetType.GER)
    ger_modeler = Modeler(ger_config)
    print("\nTraining GER model...")
    ger_cv_scores = ger_modeler.cross_validate(X_train_ger_scaled, y_train_ger)
    ger_predictions = ger_modeler.predict(X_test_ger_scaled)
    
    return {
        'eso': {
            'cv_scores': eso_cv_scores,
            'predictions': eso_predictions
        },
        'ger': {
            'cv_scores': ger_cv_scores,
            'predictions': ger_predictions
        }
    }